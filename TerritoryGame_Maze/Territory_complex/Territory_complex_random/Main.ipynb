{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ab014d3-65da-4c62-b477-08248bfa7be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sleepybear\\TerritoryGame_Maze\\Territory_complex\\Territory_complex_random\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99e1bed8-f147-428f-8c09-746f47167366",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('C:/Users/sleepybear/TerritoryGame_Maze')\n",
    "from TerritoryGame_MaPolicy import PPO  #指定路径导入一个名为TerritoryGame_MaPolicy的模块Specify the path to import a module named TerritoryGame_MaPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9af5804a-60b3-40f8-87f4-0b19e42511be",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('C:/Users/sleepybear/TerritoryGame_Maze/Territory_complex/Territory_complex_random')\n",
    "from TerritoryGame_New_Random_Env import TerritoryBattleEnvCoopRandom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03fade94-6b22-46e9-9d8b-ff1616732e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pygame\n",
    "import moviepy.editor as mpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "429ac6dd-d580-4352-952a-73cd63591641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9433c293-17ae-4f5d-82de-6a1e74ad3549",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('C:/Users/sleepybear/TerritoryGame_Maze')\n",
    "from visualization import Visualization  # Import visualization class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c221d5af-7ea8-46e0-b2e4-4fb2906519cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c2fbfde-cbb3-4e9a-b794-4b746b372dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a29531b-34b8-4df7-9118-46f53dd8a85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_policy_similarity(policy1, policy2):\n",
    "    return scipy.spatial.distance.cosine(policy1.flatten(), policy2.flatten())\n",
    "def detect_policy_switch(previous_policy, current_policy, threshold=0.1):\n",
    "    return np.linalg.norm(previous_policy - current_policy) > threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c355da38-f421-429c-9c46-90fac17e22ab",
   "metadata": {},
   "source": [
    "#This code will save the pygame video after running, which will cause 5 minutes of unresponsiveness.\n",
    "#If you don't want the video, you need to remove the code and storage list for saving the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08f7cbf7-9277-4aa9-9ff1-f1da1133d632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: C:/Users/sleepybear/TerritoryGame_Maze/Territory_complex/model/ppo_Territory_complex_model_policy_0.h5\n",
      "Loading model from: C:/Users/sleepybear/TerritoryGame_Maze/Territory_complex/model/ppo_Territory_complex_model_policy_1.h5\n",
      "Loading model from: C:/Users/sleepybear/TerritoryGame_Maze/Territory_complex/model/ppo_Territory_complex_model_policy_2.h5\n",
      "Loading model from: C:/Users/sleepybear/TerritoryGame_Maze/Territory_complex/model/ppo_Territory_complex_model_policy_3.h5\n",
      "Loading model from: C:/Users/sleepybear/TerritoryGame_Maze/Territory_complex/model/ppo_Territory_complex_model_value_0.h5\n",
      "Loading model from: C:/Users/sleepybear/TerritoryGame_Maze/Territory_complex/model/ppo_Territory_complex_model_value_1.h5\n",
      "Loading model from: C:/Users/sleepybear/TerritoryGame_Maze/Territory_complex/model/ppo_Territory_complex_model_value_2.h5\n",
      "Loading model from: C:/Users/sleepybear/TerritoryGame_Maze/Territory_complex/model/ppo_Territory_complex_model_value_3.h5\n",
      "Attacker 0 (Team red) attacks Enemy 3 (Team blue) at position (2, 0)\n",
      "Team red (Agent 0) receives +10 reward. Total reward: 9.9\n",
      "Attacker 2 (Team blue) attacks Enemy 1 (Team red) at position (6, 6)\n",
      "Team blue (Agent 2) receives +10 reward. Total reward: 9.9\n",
      "Attacker 2 (Team blue) attacks Enemy 1 (Team red) at position (0, 6)\n",
      "Team blue (Agent 2) receives +10 reward. Total reward: 10.0\n",
      "1,44.199999999999875,43.79999999999994,62,49\n",
      "\n",
      "Attacker 2 (Team blue) attacks Enemy 0 (Team red) at position (0, 6)\n",
      "Team blue (Agent 2) receives +10 reward. Total reward: 10.0\n",
      "Attacker 2 (Team blue) attacks Enemy 1 (Team red) at position (1, 6)\n",
      "Team blue (Agent 2) receives +10 reward. Total reward: 9.9\n",
      "2,10.40000000000002,80.70000000000005,29,56\n",
      "\n",
      "Attacker 2 (Team blue) attacks Enemy 0 (Team red) at position (2, 5)\n",
      "Team blue (Agent 2) receives +10 reward. Total reward: 10.0\n",
      "3,64.19999999999993,55.59999999999995,55,55\n",
      "\n",
      "Attacker 2 (Team blue) attacks Enemy 1 (Team red) at position (2, 8)\n",
      "Team blue (Agent 2) receives +10 reward. Total reward: 10.0\n",
      "4,26.899999999999952,64.9999999999999,40,53\n",
      "\n",
      "5,28.69999999999996,53.099999999999916,48,41\n",
      "\n",
      "Attacker 2 (Team blue) attacks Enemy 1 (Team red) at position (6, 9)\n",
      "Team blue (Agent 2) receives +10 reward. Total reward: 10.0\n",
      "6,28.4,43.29999999999992,78,36\n",
      "\n",
      "7,35.39999999999996,39.09999999999998,71,49\n",
      "\n",
      "8,39.8999999999999,36.69999999999997,46,50\n",
      "\n",
      "Attacker 0 (Team red) attacks Enemy 3 (Team blue) at position (8, 3)\n",
      "Team red (Agent 0) receives +10 reward. Total reward: 10.0\n",
      "9,41.99999999999996,40.399999999999885,64,52\n",
      "\n",
      "10,24.90000000000001,36.499999999999986,48,44\n",
      "\n",
      "Attacker 0 (Team red) attacks Enemy 3 (Team blue) at position (5, 5)\n",
      "Team red (Agent 0) receives +10 reward. Total reward: 9.9\n",
      "Attacker 0 (Team red) attacks Enemy 2 (Team blue) at position (1, 9)\n",
      "Team red (Agent 0) receives +10 reward. Total reward: 10.0\n",
      "Attacker 0 (Team red) attacks Enemy 3 (Team blue) at position (3, 8)\n",
      "Team red (Agent 0) receives +10 reward. Total reward: 9.9\n",
      "11,86.59999999999995,6.100000000000017,60,48\n",
      "\n",
      "Attacker 2 (Team blue) attacks Enemy 0 (Team red) at position (0, 6)\n",
      "Team blue (Agent 2) receives +10 reward. Total reward: 10.0\n",
      "12,26.69999999999996,60.89999999999995,32,64\n",
      "\n",
      "Attacker 2 (Team blue) attacks Enemy 1 (Team red) at position (0, 1)\n",
      "Team blue (Agent 2) receives +10 reward. Total reward: 10.0\n",
      "13,18.299999999999994,77.8999999999999,20,80\n",
      "\n",
      "Attacker 2 (Team blue) attacks Enemy 0 (Team red) at position (8, 4)\n",
      "Team blue (Agent 2) receives +10 reward. Total reward: 10.0\n",
      "Attacker 0 (Team red) attacks Enemy 2 (Team blue) at position (7, 4)\n",
      "Team red (Agent 0) receives +10 reward. Total reward: 10.0\n",
      "14,50.39999999999994,35.099999999999916,46,47\n",
      "\n",
      "Attacker 0 (Team red) attacks Enemy 2 (Team blue) at position (4, 1)\n",
      "Team red (Agent 0) receives +10 reward. Total reward: 9.9\n",
      "Attacker 2 (Team blue) attacks Enemy 0 (Team red) at position (10, 2)\n",
      "Team blue (Agent 2) receives +10 reward. Total reward: 10.0\n",
      "15,30.29999999999996,61.19999999999991,38,71\n",
      "\n",
      "16,58.599999999999916,50.29999999999992,70,41\n",
      "\n",
      "Attacker 0 (Team red) attacks Enemy 3 (Team blue) at position (10, 2)\n",
      "Team red (Agent 0) receives +10 reward. Total reward: 9.9\n",
      "17,67.29999999999997,28.99999999999999,76,45\n",
      "\n",
      "18,32.499999999999964,55.09999999999995,55,58\n",
      "\n",
      "Attacker 2 (Team blue) attacks Enemy 1 (Team red) at position (8, 2)\n",
      "Team blue (Agent 2) receives +10 reward. Total reward: 10.0\n",
      "Attacker 0 (Team red) attacks Enemy 3 (Team blue) at position (0, 0)\n",
      "Team red (Agent 0) receives +10 reward. Total reward: 10.0\n",
      "19,61.19999999999997,36.19999999999997,45,50\n",
      "\n",
      "Attacker 0 (Team red) attacks Enemy 3 (Team blue) at position (11, 7)\n",
      "Team red (Agent 0) receives +10 reward. Total reward: 10.0\n",
      "Attacker 0 (Team red) attacks Enemy 2 (Team blue) at position (9, 8)\n",
      "Team red (Agent 0) receives +10 reward. Total reward: 10.0\n",
      "Attacker 2 (Team blue) attacks Enemy 0 (Team red) at position (7, 4)\n",
      "Team blue (Agent 2) receives +10 reward. Total reward: 10.0\n",
      "20,62.50000000000005,26.40000000000001,56,41\n",
      "\n",
      "Attacker 2 (Team blue) attacks Enemy 1 (Team red) at position (7, 9)\n",
      "Team blue (Agent 2) receives +10 reward. Total reward: 9.9\n",
      "Attacker 0 (Team red) attacks Enemy 2 (Team blue) at position (7, 9)\n",
      "Team red (Agent 0) receives +10 reward. Total reward: 10.0\n",
      "21,32.999999999999964,59.19999999999987,46,55\n",
      "\n",
      "22,43.599999999999895,57.79999999999981,49,49\n",
      "\n",
      "23,26.69999999999998,26.599999999999962,46,29\n",
      "\n",
      "Attacker 0 (Team red) attacks Enemy 2 (Team blue) at position (3, 10)\n",
      "Team red (Agent 0) receives +10 reward. Total reward: 10.0\n",
      "24,52.69999999999995,30.799999999999933,59,52\n",
      "\n",
      "Attacker 0 (Team red) attacks Enemy 3 (Team blue) at position (10, 3)\n",
      "Team red (Agent 0) receives +10 reward. Total reward: 9.9\n",
      "25,63.49999999999986,30.39999999999999,55,57\n",
      "\n",
      "Attacker 2 (Team blue) attacks Enemy 1 (Team red) at position (7, 3)\n",
      "Team blue (Agent 2) receives +10 reward. Total reward: 9.9\n",
      "26,27.199999999999932,86.70000000000006,39,70\n",
      "\n",
      "27,76.40000000000005,40.49999999999992,52,58\n",
      "\n",
      "28,29.99999999999997,57.79999999999995,35,65\n",
      "\n",
      "29,27.49999999999999,56.59999999999981,50,42\n",
      "\n",
      "Attacker 0 (Team red) attacks Enemy 2 (Team blue) at position (0, 2)\n",
      "Team red (Agent 0) receives +10 reward. Total reward: 10.0\n",
      "30,69.29999999999987,50.09999999999986,62,63\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    env = TerritoryBattleEnvCoopRandom(grid_size=12, max_steps=150)\n",
    "    num_agents = env.num_agents\n",
    "    state_size = env.observation_space.shape[0] * env.observation_space.shape[1]\n",
    "    action_size = env.action_space.n\n",
    "    \n",
    "    ppo = PPO(state_size=state_size, action_size=action_size, num_agents=num_agents)\n",
    "    ppo.load(\"C:/Users/sleepybear/TerritoryGame_Maze/Territory_complex/model/ppo_Territory_complex_model\")\n",
    "\n",
    "    num_episodes = 30\n",
    "    batch_size = 32\n",
    "\n",
    "    vis = Visualization(env)\n",
    "\n",
    "    # 创建一个新的日志目录，每次运行时带有时间戳Create a new log directory with a timestamp each time it is run\n",
    "    log_dir = os.path.join(\"logs\", \"analysis\", f\"run_{int(time.time())}\")\n",
    "    summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "    # 存储帧的列表，用于保存视频A list of storage frames, used to save video\n",
    "    #frames = []\n",
    "\n",
    "    with open('output.txt', 'w') as file:\n",
    "        file.write(\"Episode,Total_Red_Reward,Total_Blue_Reward,Red_Territories,Blue_Territories\\n\")\n",
    "\n",
    "        win_counts = np.zeros(num_agents)\n",
    "        previous_policies = [None] * num_agents\n",
    "        policy_switches = np.zeros(num_agents)\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "            state = env.reset().flatten()\n",
    "            done = False\n",
    "            total_red_reward = 0\n",
    "            total_blue_reward = 0\n",
    "            step_count = 0\n",
    "\n",
    "            all_states, all_actions, all_rewards_per_step, all_old_probs, all_values = [], [], [], [], []\n",
    "            red_rewards, blue_rewards = [], []\n",
    "            policy_similarities = []\n",
    "\n",
    "            while not done and step_count < env.max_steps:\n",
    "                step_count += 1\n",
    "\n",
    "                actions = []\n",
    "                action_probs = []\n",
    "                policies = []\n",
    "\n",
    "                for agent_index in range(num_agents):\n",
    "                    action, policy = ppo.get_action(state, agent_index)\n",
    "                    actions.append(action)\n",
    "                    action_probs.append(policy)\n",
    "                    policies.append(policy)\n",
    "\n",
    "                next_state, rewards, done, info = env.step(actions)\n",
    "\n",
    "                all_states.append(state)\n",
    "                all_actions.append(actions)\n",
    "                all_rewards_per_step.append(np.sum(rewards))\n",
    "                all_old_probs.append(action_probs)\n",
    "\n",
    "                red_rewards.append(np.sum([reward if agent['team'] == 'red' else 0 for reward, agent in zip(rewards, env.agents)]))\n",
    "                blue_rewards.append(np.sum([reward if agent['team'] == 'blue' else 0 for reward, agent in zip(rewards, env.agents)]))\n",
    "\n",
    "                # 记录对抗结果Record the confrontation results\n",
    "                red_team_wins = info['red_count'] > info['blue_count']\n",
    "                blue_team_wins = info['blue_count'] > info['red_count']\n",
    "\n",
    "                for i, agent in enumerate(env.agents):\n",
    "                    if agent['team'] == 'red' and red_team_wins:\n",
    "                        win_counts[i] += 1\n",
    "                    elif agent['team'] == 'blue' and blue_team_wins:\n",
    "                        win_counts[i] += 1\n",
    "\n",
    "                # 计算策略相似性Calculating strategy similarity\n",
    "                for i in range(num_agents):\n",
    "                    for j in range(i + 1, num_agents):\n",
    "                        similarity = compute_policy_similarity(policies[i], policies[j])\n",
    "                        policy_similarities.append(similarity)\n",
    "\n",
    "                # 检测策略切换Detection strategy switching\n",
    "                for agent_index in range(num_agents):\n",
    "                    if previous_policies[agent_index] is not None:\n",
    "                        if detect_policy_switch(previous_policies[agent_index], policies[agent_index]):\n",
    "                            policy_switches[agent_index] += 1\n",
    "                    previous_policies[agent_index] = policies[agent_index]\n",
    "\n",
    "                for agent_index in range(num_agents):\n",
    "                    value = ppo.value_models[agent_index](tf.convert_to_tensor(state[None, :])).numpy()[0]\n",
    "                    all_values.append(value)\n",
    "\n",
    "                state = next_state.flatten()\n",
    "                total_red_reward += np.sum([reward if agent['team'] == 'red' else 0 for reward, agent in zip(rewards, env.agents)])\n",
    "                total_blue_reward += np.sum([reward if agent['team'] == 'blue' else 0 for reward, agent in zip(rewards, env.agents)])\n",
    "\n",
    "                vis.update(episode)\n",
    "                env.handle_events()\n",
    "\n",
    "                # 捕获当前帧并添加到帧列表Capture the current frame and add it to the frame list\n",
    "                #frame = pygame.surfarray.array3d(pygame.display.get_surface())\n",
    "                #frame = frame.transpose([1, 0, 2])\n",
    "                #frames.append(frame)\n",
    "\n",
    "            if (episode + 1) % batch_size == 0:\n",
    "                with summary_writer.as_default():\n",
    "                    for agent_index in range(num_agents):\n",
    "                        tf.summary.scalar(f\"Agent_{agent_index}_Reward\", total_red_reward if env.agents[agent_index]['team'] == 'red' else total_blue_reward, step=episode)\n",
    "                        tf.summary.scalar(f\"Agent_{agent_index}_Win_Rate\", win_counts[agent_index] / (episode + 1), step=episode)\n",
    "                        tf.summary.scalar(f\"Agent_{agent_index}_Policy_Switches\", policy_switches[agent_index], step=episode)\n",
    "                    tf.summary.scalar(f\"Average_Policy_Similarity\", np.mean(policy_similarities), step=episode)\n",
    "\n",
    "            output = (\n",
    "                f\"{episode + 1},{total_red_reward},{total_blue_reward},\"\n",
    "                f\"{info['red_count']},{info['blue_count']}\\n\"\n",
    "            )\n",
    "            print(output)\n",
    "            logging.info(f\"Episode {episode + 1} completed: {output.strip()}\")\n",
    "            file.write(output)\n",
    "\n",
    "        # 保存帧列表为视频 Save frame list as video\n",
    "        #clip = mpy.ImageSequenceClip(frames, fps=30)\n",
    "        #clip.write_videofile(\"pygame_recording.mp4\", codec=\"libx264\")\n",
    "\n",
    "        vis.show()\n",
    "        summary_writer.close()\n",
    "        logging.info(\"Training completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except SystemExit:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb72c3e7-2e31-4af3-8767-ad19acd3ff01",
   "metadata": {},
   "source": [
    "#If you want to use tensorboard, you must first clear the files in the analysis folder to ensure that there is only one curve.\n",
    "#If you do not clear it, the results of multiple trainings in different timelines will be displayed.\n",
    "#You also need to end the previous tensorboard process in the task manager, otherwise the server will still display the last training results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbc5356-ab1e-4e1f-accc-f15b040ebb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 启动 TensorBoard\n",
    "!tensorboard --logdir logs/analysis --port=6010\n",
    "#http://localhost:6010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9aeca6-8239-4a26-9e1b-8ad004abf549",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gym_env)",
   "language": "python",
   "name": "gym_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
